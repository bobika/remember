---
layout:     post
title:      "Python爬虫"
subtitle:   " \"定向网络数据爬取和网页解析\""
date:       2018-02-05
author:     "Bobika"
header-img: "img/Hello-bg.jpg"
catalog: true
tags:
    - Tech
    - Python
    - 爬虫
---
# Python 爬虫
**本文属于学习笔记**
>Requests库：实现自动爬取HTML页面，自动进行网络内容的提交
>robots.txt：网络爬虫排除标准
>Projects：实战项目（使用request库）

----

## 知识必备

**HTTP协议**
HTTP，Hypertext Teansfer Protocol，超文本传输协议。<br>
HTTP协议是一个基于“请求与响应”模式的、无状态的应用层协议。（什么是应用层协议，应用层协议都有啥就再去补网络的知识吧，muma）<br>
HTTP协议采用URL作为定位网络资源的标识。<br>
URL格式如下：http://host[:port\]\[path\]<br>
host：合法的Internet主机域名或IP地址<br>
port：端口号，缺省端口为80<br>
path：请求资源的路径<br>

**HTTP协议对资源的主要操作**
> requests库的方法和HTTP协议操作一一对应<br>
> 方法|说明
> ----|----
> GET|请求获取URL位置的资源
> HEAD|请求获取URL位置资源的响应消息报告，即获得该资源的头部信息
> POST|请求向URL位置的资源后附加新的数据
> PUT|请求向URL位置存储一个资源没覆盖原URL位置的资源
> PATCH|请求局部更新URL位置的资源，即改变该处资源的部分内容
> DELETE|请求删除URL位置存储的资源

## Request库
[Request库](http://www.python-requests.org)为Python的第三方库，目前公认的爬取网站最好的第三方库。
**Request库的安装**
打开命令行输入`pip install requests`即可
> 测试：其中`r.status_code`为输出状态码，200表示获取成功<br>
 <img class="shadow" src="img/in-post/post-python/pp1-1.png" />
 ![request](img/in-post/post-python/pp1-1.png)

**Requests库的7个主要方法**
requests()方法为基础方法，其他方法都通过调用requests()方法来实现。
> 方法|说明
> ----|----
> requests.request()|构造一个请求，支撑以下各方法的基础方法
> requests.get()|获取HTML网页的主要方法，对应于HTTP的GET
> requests.head()|获取HTML网页头信息的方法，对应于HTTP的HEAD
> requests.post()|向HTML网页提交POST请求的方法，对应于HTTP的POST
> requests.put()|向HTML页面提交PUT请求的方法，对应于HTTP的PUT
> requests.patch()|向HTML页面提交局部修改请求，对应于HTTP的PATCH
> requests.delete()|向HTML页面提交删除请求，对应于HTTP的DELETE

**Requests库的6种常用异常**
> 异常|说明
> ----|----
> requests.ConnectionError|网络连接错误异常，如DNS查询失败、拒绝连接等
> requests.HTTPError|HTTP错误异常
> requests.URLRequired|URL缺失异常
> requests.TooManyRedirects|超过最大重定向次数，产生重定向异常
> requests.ConnectTimeout|连接远程服务器超时异常
> requests.Timeout|请求URL超时，产生超时异常(从发出URL开始到整个过程的超时异常)

**request方法**
>完整的request函数：`requests.request(method, url, **kwargs)` <br>
>其中：
>method为请求方式，有七种:
>`'GET','HEAD','POST','PUT','PATCH','delete','OPTIONS'`<br>
>**kwargs为控制访问的参数，均可作为可选项，可选项有：<br>
> 1. params：字典或字节序列，作为参数增加到url中
> 2. data：字典、字节序列或文件对象，作为Request的内容
> 3. json：JSON格式的数据，作为Request的内容
> 4. headers：字典，HTTP定制头
> 5. cookies：字典或CookieJar，Request中的cookie
> 6. auth：元组，支持HTTP认证功能
> 7. files：字典类型，传输文件
> 8. timeout：设置超时时间，秒为单位
> 9. proxies：字典类型，设定访问代理服务器，可以增加登录认证
> 10. allow_redirects：True/False，默认为True，重定向开关
> 11. stream：True/False，默认为True，获取内容立即下载开关
> 12. verify：True/False，默认为True，认证SSL整数开关
> 13. cert：本地SSL证书路径

**get方法**
最简单的获取网页信息的代码：`r = requests.get(url)`<br>
它构造一个向服务器请求资源的**Request**对象，返回一个包含服务器资源**Response**对象（如上述语句中，返回的Response对象就赋给了r），而Response对象中，包含从服务器返回的所有的相关资源（即爬虫返回的内容）。<br>
>完整的get函数：`requests.get(url, params=None, **kwargs)`<br>
>1. url：拟获取页面的url链接
>2. params：url中的额外参数，字典或字节流格式，可选
>3. **kwargs：12个控制访问的参数，request中除了params的其他12个

**head方法**
>完整的head函数：`requests.head(url, **kwargs)`<br>
>url：拟获取页面的url链接
>**kwargs:13个控制访问的参数

**post方法**
>完整的post函数：`requests.post(url, data=None，json=None，**kwargs)`<br>
>url：拟更新页面的url
>data：字典、字节序列或文件，Request的内容
>json：JSON格式的数据，Request的内容
>**kwargs：11个控制访问的参数

**put方法**
>完整的put函数：`requests.put(url, data=None，**kwargs)`<br>
>url：拟更新页面的url  
>data：字典、字节序列或文件，Request的内容
>**kwargs：12个控制访问的参数

**patch方法**
>完整的post函数：`requests.post(url, data=None，**kwargs)`<br>
>url：拟更新页面的url
>data：字典、字节序列或文件，Request的内容
>**kwargs：12个控制访问的参数

**delete方法**
>完整的post函数：`requests.post(url, **kwargs)`<br>
>url：拟删除页面的url
>**kwargs：13个控制访问的参数


**Response对象的属性**
> 属性|说明
> ----|----
> r.status_code|HHTTP请求的返回状态，200表示连接成功，404表示失效（只要不是200）
> r.text|HTTP相应内容的字符串形式，即，url对应的页面内容
> r.encoding|从HTTP header中 **猜测** 的响应内容编码方式
> r.apparent_encoding|从内容中分析出的响应内容编码方式（备选编码方式）
> r.content|HTTP响应内容的二进制形式

**Response对象提供的方法**
> 异常|说明
> ----|----
> r.raise_for_status()|如果不是200，产生异常requests.HTTPError

## 爬取网页的通用代码框架
```Python
import requests 
def getHTMLText(url):
    try:
       r = requests.get(url, timeout=30)
       r.raise_for_status() #如果状态不是200，引发HTTPError异常
       r.encoding = r.apparent_encoding
       return r.text
    except:
       return "产生异常"

if __name__ == "__main__":
    url = input("Plese input your url:")
    print(getHTMLText(url))
```

## 网络爬虫的尺寸
1. 爬取网页，玩网页----小规模，数据量小，爬取速度不敏感的。----Requests库----使用>90%
2. 爬取网站，爬取系列网站----中规模，数据规模较大，爬取速度敏感。----Scrapy库
3. 爬取全网----大规模，搜索引擎，爬取速度关键----定制开发

## Robots协议
Robots Exclasion Standard 网络爬虫排除标准<br>
作用：网站告知网络爬虫哪些页面可以抓取，哪些不行。<br>
形式：在网站根目录下的Robots.txt文件。<br>

**案例：京东的Robots协议**
```http://www.jd.com/robots.txt
User-agent: *                 /*指对任何的网络爬虫来源，*代表所有爬虫 */
Disallow: /?*                 /*任何爬虫都不允许访问？*的内容*/
Disallow: /pop/*.html         /*任何爬虫都不允许访问/pop/*.html */
Disallow: /pinpai/*.html?* 
User-agent: EtaoSpider        /* 以下四个爬虫被禁止了*/
Disallow: / 
User-agent: HuihuiSpider 
Disallow: / 
User-agent: GwdangSpider 
Disallow: / 
User-agent: WochachaSpider 
Disallow: /
```

**Robots协议遵守**
网络爬虫：自动或人工识别robots.txt,再进行内容爬取。

## 实例部分
1. 京东商品页面的爬取
2. 亚马逊商品页面的爬取
3. 百度360搜索关键词提交
4. 网络图片的爬取和存储
5. IP地址归属地的自动查询